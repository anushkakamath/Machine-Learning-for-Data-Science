{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84e10c09",
   "metadata": {},
   "source": [
    "### Name : Anushka Kamath\n",
    "### Github Username : anushkakamath\n",
    "### USC email id : arkamath@usc.edu\n",
    "### USC ID : 9418501008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f710098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d46679",
   "metadata": {},
   "source": [
    "# 1. (a) \n",
    "Download the AReM data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d6088b",
   "metadata": {},
   "source": [
    "# 1. (b)\n",
    "Keep datasets 1 and 2 in folders bending1 and bending 2, as well as datasets 1, 2, and 3 in other folders as test data and other datasets as train data.\n",
    "\n",
    "https://www.codegrepper.com/code-examples/python/iterate+over+folders+python\n",
    "\n",
    "digits from str using regex :\n",
    "https://stackoverflow.com/questions/4289331/how-to-extract-numbers-from-a-string-in-python\n",
    "https://www.askpython.com/python/string/extract-digits-from-python-string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53c757bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../data/AReM/\"\n",
    "train_files = []\n",
    "test_files = []\n",
    "\n",
    "for root, subdirectories, files in os.walk(directory):\n",
    "\n",
    "    if len(subdirectories) == 0:\n",
    "        #print(root)\n",
    "        folder_name = os.path.split(root)[1]\n",
    "        #print(\"sub \", subdirectories)\n",
    "        for file in files:\n",
    "            file_num = re.findall(\"\\d+\", file)[0]\n",
    "            file_path = os.path.join(root, file)\n",
    "            if folder_name in [\"bending1\", \"bending2\"]:\n",
    "                if int(file_num) < 3:\n",
    "                    test_files.append(file_path)\n",
    "                else:\n",
    "                    train_files.append(file_path)\n",
    "            else:\n",
    "                if int(file_num) < 4:\n",
    "                    test_files.append(file_path)\n",
    "                else:\n",
    "                    train_files.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a64b86a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have :  69 train files \n",
      "\n",
      "Train data files -  ['../data/AReM/bending1/dataset7.csv', '../data/AReM/bending1/dataset6.csv', '../data/AReM/bending1/dataset4.csv', '../data/AReM/bending1/dataset5.csv', '../data/AReM/bending1/dataset3.csv', '../data/AReM/walking/dataset7.csv', '../data/AReM/walking/dataset6.csv', '../data/AReM/walking/dataset4.csv', '../data/AReM/walking/dataset5.csv', '../data/AReM/walking/dataset10.csv', '../data/AReM/walking/dataset11.csv', '../data/AReM/walking/dataset13.csv', '../data/AReM/walking/dataset12.csv', '../data/AReM/walking/dataset15.csv', '../data/AReM/walking/dataset14.csv', '../data/AReM/walking/dataset8.csv', '../data/AReM/walking/dataset9.csv', '../data/AReM/bending2/dataset6.csv', '../data/AReM/bending2/dataset4.csv', '../data/AReM/bending2/dataset5.csv', '../data/AReM/bending2/dataset3.csv', '../data/AReM/standing/dataset7.csv', '../data/AReM/standing/dataset6.csv', '../data/AReM/standing/dataset4.csv', '../data/AReM/standing/dataset5.csv', '../data/AReM/standing/dataset10.csv', '../data/AReM/standing/dataset11.csv', '../data/AReM/standing/dataset13.csv', '../data/AReM/standing/dataset12.csv', '../data/AReM/standing/dataset15.csv', '../data/AReM/standing/dataset14.csv', '../data/AReM/standing/dataset8.csv', '../data/AReM/standing/dataset9.csv', '../data/AReM/sitting/dataset7.csv', '../data/AReM/sitting/dataset6.csv', '../data/AReM/sitting/dataset4.csv', '../data/AReM/sitting/dataset5.csv', '../data/AReM/sitting/dataset10.csv', '../data/AReM/sitting/dataset11.csv', '../data/AReM/sitting/dataset13.csv', '../data/AReM/sitting/dataset12.csv', '../data/AReM/sitting/dataset15.csv', '../data/AReM/sitting/dataset14.csv', '../data/AReM/sitting/dataset8.csv', '../data/AReM/sitting/dataset9.csv', '../data/AReM/lying/dataset7.csv', '../data/AReM/lying/dataset6.csv', '../data/AReM/lying/dataset4.csv', '../data/AReM/lying/dataset5.csv', '../data/AReM/lying/dataset10.csv', '../data/AReM/lying/dataset11.csv', '../data/AReM/lying/dataset13.csv', '../data/AReM/lying/dataset12.csv', '../data/AReM/lying/dataset15.csv', '../data/AReM/lying/dataset14.csv', '../data/AReM/lying/dataset8.csv', '../data/AReM/lying/dataset9.csv', '../data/AReM/cycling/dataset7.csv', '../data/AReM/cycling/dataset6.csv', '../data/AReM/cycling/dataset4.csv', '../data/AReM/cycling/dataset5.csv', '../data/AReM/cycling/dataset10.csv', '../data/AReM/cycling/dataset11.csv', '../data/AReM/cycling/dataset13.csv', '../data/AReM/cycling/dataset12.csv', '../data/AReM/cycling/dataset15.csv', '../data/AReM/cycling/dataset14.csv', '../data/AReM/cycling/dataset8.csv', '../data/AReM/cycling/dataset9.csv']\n"
     ]
    }
   ],
   "source": [
    "print(\"We have : \", len(train_files), \"train files \\n\")\n",
    "print(\"Train data files - \", train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4309f276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have :  19 test files \n",
      "\n",
      "Test data files -  ['../data/AReM/bending1/dataset1.csv', '../data/AReM/bending1/dataset2.csv', '../data/AReM/walking/dataset1.csv', '../data/AReM/walking/dataset2.csv', '../data/AReM/walking/dataset3.csv', '../data/AReM/bending2/dataset1.csv', '../data/AReM/bending2/dataset2.csv', '../data/AReM/standing/dataset1.csv', '../data/AReM/standing/dataset2.csv', '../data/AReM/standing/dataset3.csv', '../data/AReM/sitting/dataset1.csv', '../data/AReM/sitting/dataset2.csv', '../data/AReM/sitting/dataset3.csv', '../data/AReM/lying/dataset1.csv', '../data/AReM/lying/dataset2.csv', '../data/AReM/lying/dataset3.csv', '../data/AReM/cycling/dataset1.csv', '../data/AReM/cycling/dataset2.csv', '../data/AReM/cycling/dataset3.csv']\n"
     ]
    }
   ],
   "source": [
    "print(\"We have : \", len(test_files), \"test files \\n\")\n",
    "print(\"Test data files - \", test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20385e4",
   "metadata": {},
   "source": [
    "# 1. (c) Feature Extraction\n",
    "Classification of time series usually needs extracting features from them. In this problem, we focus on time-domain features.\n",
    "\n",
    "i. Research what types of time-domain features are usually used in time series classification and list them (examples are minimum, maximum, mean, etc).\n",
    "\n",
    "ANS : Time series data, also referred to as time-stamped data, is a sequence of data points indexed in time order. Other than the fundamental min, max, mean, standard deviation, first quartile, and third quartile properties. Distribution, Correlation structure, Entropy, Scaling and Stationarity properties are typically utilized in time series categorization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b78164",
   "metadata": {},
   "source": [
    "ii. Extract the time-domain features minimum, maximum, mean, median, stan- dard deviation, first quartile, and third quartile for all of the 6 time series in each instance. You are free to normalize/standardize features or use them directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92244ef8",
   "metadata": {},
   "source": [
    "https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n",
    "\n",
    "https://stackoverflow.com/questions/18039057/python-pandas-error-tokenizing-data\n",
    "\n",
    "https://www.geeksforgeeks.org/how-to-create-an-empty-dataframe-and-append-rows-columns-to-it-in-pandas/\n",
    "\n",
    "https://www.geeksforgeeks.org/python-pandas-dataframe-append/\n",
    "\n",
    "https://stackoverflow.com/questions/19851005/rename-pandas-dataframe-index\n",
    "https://note.nkmk.me/en/python-pandas-dataframe-rename/\n",
    "\n",
    "https://thispointer.com/how-to-drop-index-column-of-a-pandas-dataframe/\n",
    "\n",
    "Filter warnings :\n",
    "https://stackoverflow.com/questions/14463277/how-to-disable-python-warnings\n",
    "https://docs.python.org/3/library/warnings.html\n",
    "\n",
    "reorder columns :\n",
    "https://datagy.io/reorder-pandas-columns/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc43c1",
   "metadata": {},
   "source": [
    "-- minor changes in the below dataset in excel to remove extra comma \n",
    "\n",
    "AReM/cycling/dataset9.csv -> Error tokenizing data. C error: Expected 7 fields in line 485, saw 8\n",
    "\n",
    "AReM/cycling/dataset14.csv -> Error tokenizing data. C error: Expected 7 fields in line 485, saw 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d4f2c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/AReM/bending1/dataset7.csv\n",
      "../data/AReM/bending1/dataset6.csv\n",
      "../data/AReM/bending1/dataset4.csv\n",
      "../data/AReM/bending1/dataset5.csv\n",
      "../data/AReM/bending1/dataset3.csv\n",
      "../data/AReM/walking/dataset7.csv\n",
      "../data/AReM/walking/dataset6.csv\n",
      "../data/AReM/walking/dataset4.csv\n",
      "../data/AReM/walking/dataset5.csv\n",
      "../data/AReM/walking/dataset10.csv\n",
      "../data/AReM/walking/dataset11.csv\n",
      "../data/AReM/walking/dataset13.csv\n",
      "../data/AReM/walking/dataset12.csv\n",
      "../data/AReM/walking/dataset15.csv\n",
      "../data/AReM/walking/dataset14.csv\n",
      "../data/AReM/walking/dataset8.csv\n",
      "../data/AReM/walking/dataset9.csv\n",
      "../data/AReM/bending2/dataset6.csv\n",
      "../data/AReM/bending2/dataset4.csv\n",
      "../data/AReM/bending2/dataset5.csv\n",
      "../data/AReM/bending2/dataset3.csv\n",
      "../data/AReM/standing/dataset7.csv\n",
      "../data/AReM/standing/dataset6.csv\n",
      "../data/AReM/standing/dataset4.csv\n",
      "../data/AReM/standing/dataset5.csv\n",
      "../data/AReM/standing/dataset10.csv\n",
      "../data/AReM/standing/dataset11.csv\n",
      "../data/AReM/standing/dataset13.csv\n",
      "../data/AReM/standing/dataset12.csv\n",
      "../data/AReM/standing/dataset15.csv\n",
      "../data/AReM/standing/dataset14.csv\n",
      "../data/AReM/standing/dataset8.csv\n",
      "../data/AReM/standing/dataset9.csv\n",
      "../data/AReM/sitting/dataset7.csv\n",
      "../data/AReM/sitting/dataset6.csv\n",
      "../data/AReM/sitting/dataset4.csv\n",
      "../data/AReM/sitting/dataset5.csv\n",
      "../data/AReM/sitting/dataset10.csv\n",
      "../data/AReM/sitting/dataset11.csv\n",
      "../data/AReM/sitting/dataset13.csv\n",
      "../data/AReM/sitting/dataset12.csv\n",
      "../data/AReM/sitting/dataset15.csv\n",
      "../data/AReM/sitting/dataset14.csv\n",
      "../data/AReM/sitting/dataset8.csv\n",
      "../data/AReM/sitting/dataset9.csv\n",
      "../data/AReM/lying/dataset7.csv\n",
      "../data/AReM/lying/dataset6.csv\n",
      "../data/AReM/lying/dataset4.csv\n",
      "../data/AReM/lying/dataset5.csv\n",
      "../data/AReM/lying/dataset10.csv\n",
      "../data/AReM/lying/dataset11.csv\n",
      "../data/AReM/lying/dataset13.csv\n",
      "../data/AReM/lying/dataset12.csv\n",
      "../data/AReM/lying/dataset15.csv\n",
      "../data/AReM/lying/dataset14.csv\n",
      "../data/AReM/lying/dataset8.csv\n",
      "../data/AReM/lying/dataset9.csv\n",
      "../data/AReM/cycling/dataset7.csv\n",
      "../data/AReM/cycling/dataset6.csv\n",
      "../data/AReM/cycling/dataset4.csv\n",
      "../data/AReM/cycling/dataset5.csv\n",
      "../data/AReM/cycling/dataset10.csv\n",
      "../data/AReM/cycling/dataset11.csv\n",
      "../data/AReM/cycling/dataset13.csv\n",
      "../data/AReM/cycling/dataset12.csv\n",
      "../data/AReM/cycling/dataset15.csv\n",
      "../data/AReM/cycling/dataset14.csv\n",
      "../data/AReM/cycling/dataset8.csv\n",
      "../data/AReM/cycling/dataset9.csv\n",
      "../data/AReM/bending1/dataset1.csv\n",
      "../data/AReM/bending1/dataset2.csv\n",
      "../data/AReM/walking/dataset1.csv\n",
      "../data/AReM/walking/dataset2.csv\n",
      "../data/AReM/walking/dataset3.csv\n",
      "../data/AReM/bending2/dataset1.csv\n",
      "../data/AReM/bending2/dataset2.csv\n",
      "../data/AReM/standing/dataset1.csv\n",
      "../data/AReM/standing/dataset2.csv\n",
      "../data/AReM/standing/dataset3.csv\n",
      "../data/AReM/sitting/dataset1.csv\n",
      "../data/AReM/sitting/dataset2.csv\n",
      "../data/AReM/sitting/dataset3.csv\n",
      "../data/AReM/lying/dataset1.csv\n",
      "../data/AReM/lying/dataset2.csv\n",
      "../data/AReM/lying/dataset3.csv\n",
      "../data/AReM/cycling/dataset1.csv\n",
      "../data/AReM/cycling/dataset2.csv\n",
      "../data/AReM/cycling/dataset3.csv\n"
     ]
    }
   ],
   "source": [
    "time1 = pd.DataFrame()\n",
    "time2 = pd.DataFrame()\n",
    "time3 = pd.DataFrame()\n",
    "time4 = pd.DataFrame()\n",
    "time5 = pd.DataFrame()\n",
    "time6 = pd.DataFrame()\n",
    "\n",
    "for file in (train_files + test_files):\n",
    "    #df = pd.read_csv(file, skiprows = 4, on_bad_lines='skip')\n",
    "    #removed bad lines by removing excess commas in excel\n",
    "    \n",
    "    df = pd.read_csv(file, skiprows = 4)\n",
    "    summary = df.describe()\n",
    "    \n",
    "    print(file)\n",
    "    avg12 = df.describe()['avg_rss12']\n",
    "    avg12 = avg12.rename(index = {'min': 'min1', 'max': 'max1', 'mean': 'mean1', '50%': 'median1', 'std': 'std1', '25%': '1st quart1', '75%': '3rd quart1'})\n",
    "    avg12 = avg12.drop(index = ['count'])\n",
    "    avg12 = avg12[['min1', 'max1', 'mean1',  'median1', 'std1', '1st quart1', '3rd quart1']]\n",
    "    time1 = time1.append(avg12, ignore_index = True)\n",
    "    \n",
    "    \n",
    "    var12 = df.describe()['var_rss12']\n",
    "    var12 = var12.rename(index = {'min': 'min2', 'max': 'max2', 'mean': 'mean2', '50%': 'median2', 'std': 'std2', '25%': '1st quart2', '75%': '3rd quart2'})\n",
    "    var12 = var12.drop(index = ['count'])\n",
    "    var12 = var12[['min2', 'max2', 'mean2',  'median2', 'std2', '1st quart2', '3rd quart2']]\n",
    "    time2 = time2.append(var12, ignore_index = True)\n",
    "    \n",
    "    \n",
    "    avg13 = df.describe()['avg_rss13']\n",
    "    avg13 = avg13.rename(index = {'min': 'min3', 'max': 'max3', 'mean': 'mean3', '50%': 'median3', 'std': 'std3', '25%': '1st quart3', '75%': '3rd quart3'})\n",
    "    avg13 = avg13.drop(index = ['count'])\n",
    "    avg13 = avg13[['min3', 'max3', 'mean3',  'median3', 'std3', '1st quart3', '3rd quart3']]\n",
    "    time3 = time3.append(avg13, ignore_index = True)\n",
    "    \n",
    "    \n",
    "    var13 = df.describe()['var_rss13']\n",
    "    var13 = var13.rename(index = {'min': 'min4', 'max': 'max4', 'mean': 'mean4', '50%': 'median4', 'std': 'std4', '25%': '1st quart4', '75%': '3rd quart4'})\n",
    "    var13 = var13.drop(index = ['count'])\n",
    "    var13 = var13[['min4', 'max4', 'mean4',  'median4', 'std4', '1st quart4', '3rd quart4']]\n",
    "    time4 = time4.append(var13, ignore_index = True)\n",
    "    \n",
    "    \n",
    "    avg23 = df.describe()['avg_rss23']\n",
    "    avg23 = avg23.rename(index = {'min': 'min5', 'max': 'max5', 'mean': 'mean5', '50%': 'median5', 'std': 'std5', '25%': '1st quart5', '75%': '3rd quart5'})\n",
    "    avg23 = avg23.drop(index = ['count'])\n",
    "    avg23 = avg23[['min5', 'max5', 'mean5',  'median5', 'std5', '1st quart5', '3rd quart5']]\n",
    "    time5 = time5.append(avg23, ignore_index = True)\n",
    "    \n",
    "    \n",
    "    var23 = df.describe()['var_rss23']\n",
    "    var23 = var23.rename(index = {'min': 'min6', 'max': 'max6', 'mean': 'mean6', '50%': 'median6', 'std': 'std6', '25%': '1st quart6', '75%': '3rd quart6'})\n",
    "    var23 = var23.drop(index = ['count'])\n",
    "    var23 = var23[['min6', 'max6', 'mean6',  'median6', 'std6', '1st quart6', '3rd quart6']]\n",
    "    time6 = time6.append(var23, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c203e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['column name'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b43ed20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time-domain features for avg rss12 is :\n",
      "      min1   max1      mean1  median1      std1  1st quart1  3rd quart1\n",
      "0   36.25  48.00  43.969125   44.500  1.618364       43.31     44.6700\n",
      "1   37.00  48.00  43.454958   43.250  1.386098       42.50     45.0000\n",
      "2   33.00  47.75  42.179813   43.500  3.670666       39.15     45.0000\n",
      "3   33.00  45.75  41.678063   41.750  2.243490       41.33     42.7500\n",
      "4   35.00  47.40  43.954500   44.330  1.558835       43.00     45.0000\n",
      "..    ...    ...        ...      ...       ...         ...         ...\n",
      "83  24.75  48.33  44.182937   48.000  7.495615       48.00     48.0000\n",
      "84  48.00  48.25  48.004167   48.000  0.032038       48.00     48.0000\n",
      "85  24.25  45.00  37.177042   36.250  3.581301       34.50     40.2500\n",
      "86  28.75  44.75  37.561188   36.875  3.226507       35.25     40.2500\n",
      "87  22.00  44.67  37.058708   36.000  3.710180       34.50     40.0625\n",
      "\n",
      "[88 rows x 7 columns] \n",
      "\n",
      "time-domain features for var rss12 is :\n",
      "     min2   max2     mean2  median2      std2  1st quart2  3rd quart2\n",
      "0    0.0   1.50  0.413125     0.47  0.263111        0.43        0.50\n",
      "1    0.0   1.58  0.378083     0.47  0.315566        0.00        0.50\n",
      "2    0.0   3.00  0.696042     0.50  0.630860        0.00        1.12\n",
      "3    0.0   2.83  0.535979     0.50  0.405469        0.43        0.71\n",
      "4    0.0   1.70  0.426250     0.47  0.338690        0.00        0.50\n",
      "..   ...    ...       ...      ...       ...         ...         ...\n",
      "83   0.0   3.11  0.101875     0.00  0.346756        0.00        0.00\n",
      "84   0.0   0.43  0.007167     0.00  0.055106        0.00        0.00\n",
      "85   0.0   8.58  2.374208     1.92  1.601799        1.30        3.13\n",
      "86   0.0   9.91  2.080688     1.70  1.639258        1.12        2.87\n",
      "87   0.0  14.17  2.438146     1.92  1.996887        1.25        3.35\n",
      "\n",
      "[88 rows x 7 columns] \n",
      "\n",
      "time-domain features for avg rss13 is :\n",
      "     min3   max3      mean3  median3      std3  1st quart3  3rd quart3\n",
      "0   1.50  26.33  15.868021   16.250  3.742420       14.25     18.0000\n",
      "1   5.75  27.00  15.793333   15.000  3.847638       13.00     18.2700\n",
      "2   8.50  30.00  22.183625   23.000  3.810469       20.50     24.3725\n",
      "3   3.00  28.25  19.006562   19.125  4.087107       16.50     22.0625\n",
      "4   6.50  29.75  22.122354   23.000  3.030943       19.75     24.0000\n",
      "..   ...    ...        ...      ...       ...         ...         ...\n",
      "83  1.00  16.50   6.679958    6.250  1.936492        5.67      7.5000\n",
      "84  0.00  13.00   4.900563    5.500  2.566429        3.00      6.2500\n",
      "85  5.50  26.75  16.531083   16.670  3.430906       14.25     19.0000\n",
      "86  6.50  24.67  16.567042   17.000  3.691401       14.00     19.5000\n",
      "87  6.33  24.00  16.388312   16.500  3.537950       13.75     19.0000\n",
      "\n",
      "[88 rows x 7 columns] \n",
      "\n",
      "time-domain features for var rss13 is :\n",
      "     min4   max4     mean4  median4      std4  1st quart4  3rd quart4\n",
      "0    0.0   5.17  0.666354     0.47  0.788985        0.00      0.9400\n",
      "1    0.0  10.03  0.849354     0.50  0.995761        0.43      1.1200\n",
      "2    0.0   5.15  0.989917     0.83  0.953730        0.43      1.3000\n",
      "3    0.0   6.42  0.841875     0.50  0.928801        0.43      1.1200\n",
      "4    0.0   4.44  0.497313     0.43  0.550657        0.00      0.8300\n",
      "..   ...    ...       ...      ...       ...         ...         ...\n",
      "83   0.0   5.91  0.584104     0.43  0.749945        0.00      0.7100\n",
      "84   0.0   2.86  0.397313     0.43  0.447127        0.00      0.5000\n",
      "85   0.0   8.05  2.910604     2.62  1.600137        1.64      3.9175\n",
      "86   0.0   8.32  3.033875     2.95  1.625415        1.79      4.0300\n",
      "87   0.0   9.74  2.980688     2.86  1.612059        1.79      4.0000\n",
      "\n",
      "[88 rows x 7 columns] \n",
      "\n",
      "time-domain features for avg rss13 is :\n",
      "      min5   max5      mean5  median5      std5  1st quart5  3rd quart5\n",
      "0   11.33  30.75  22.103750   21.670  3.318301     20.5000     23.7500\n",
      "1    8.00  33.50  23.034792   23.500  2.488862     22.2500     24.0000\n",
      "2   20.00  38.67  33.493917   35.000  3.849448     30.4575     36.3300\n",
      "3   23.67  37.50  29.857083   30.000  2.411026     28.4575     31.2500\n",
      "4   29.00  38.50  35.588458   36.000  1.999604     35.3625     36.5000\n",
      "..    ...    ...        ...      ...       ...         ...         ...\n",
      "83   0.00  12.75   4.376292    3.330  3.274539      2.0000      5.5425\n",
      "84   0.00  12.00   7.624896    9.000  3.268502      4.6700     10.0000\n",
      "85   7.00  25.50  19.607250   20.000  2.890347     17.9500     21.7500\n",
      "86  10.50  24.33  19.518896   20.000  2.727377     18.0000     21.5000\n",
      "87   7.50  24.25  18.125958   18.875  3.537144     16.0000     21.0000\n",
      "\n",
      "[88 rows x 7 columns] \n",
      "\n",
      "time-domain features for ar rss23 is :\n",
      "     min6  max6     mean6  median6      std6  1st quart6  3rd quart6\n",
      "0    0.0  2.96  0.555313     0.49  0.487826      0.0000        0.83\n",
      "1    0.0  5.26  0.679646     0.50  0.622534      0.4300        0.87\n",
      "2    0.0  2.18  0.613521     0.50  0.524317      0.0000        1.00\n",
      "3    0.0  1.79  0.383292     0.43  0.389164      0.0000        0.50\n",
      "4    0.0  1.79  0.493292     0.43  0.513506      0.0000        0.94\n",
      "..   ...   ...       ...      ...       ...         ...         ...\n",
      "83   0.0  3.91  0.692771     0.50  0.675781      0.3225        0.94\n",
      "84   0.0  2.50  0.641229     0.50  0.388372      0.4600        0.83\n",
      "85   0.0  9.34  2.921729     2.50  1.852600      1.5000        3.90\n",
      "86   0.0  9.62  2.765896     2.45  1.769203      1.4100        3.77\n",
      "87   0.0  8.55  2.983750     2.57  1.815730      1.5000        4.15\n",
      "\n",
      "[88 rows x 7 columns] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"time-domain features for avg rss12 is :\\n\" , time1, \"\\n\")\n",
    "print(\"time-domain features for var rss12 is :\\n\" , time2, \"\\n\")\n",
    "print(\"time-domain features for avg rss13 is :\\n\" , time3, \"\\n\")\n",
    "print(\"time-domain features for var rss13 is :\\n\" , time4, \"\\n\")\n",
    "print(\"time-domain features for avg rss13 is :\\n\" , time5, \"\\n\")\n",
    "print(\"time-domain features for ar rss23 is :\\n\" , time6, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bec839",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/28773683/combine-two-pandas-dataframes-with-the-same-index\n",
    "\n",
    "https://towardsdatascience.com/pandas-concat-tricks-you-should-know-to-speed-up-your-data-analysis-cd3d4fdfe6dd#:~:text=Dealing%20with%20index%20and%20axis,-Suppose%20we%20have&text=If%20you%20want%20the%20concatenation,%2C%20%E2%80%A6%2C%20n%2D1%20.&text=To%20concatenate%20DataFrames%20horizontally%20along,set%20the%20argument%20axis%3D1%20.\n",
    "\n",
    "https://www.geeksforgeeks.org/python-pandas-dataframe-append/\n",
    "\n",
    "https://datagy.io/pandas-drop-index-column/#:~:text=The%20most%20straightforward%20way%20to,a%20column%20in%20the%20dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc735b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min1         NaN\n",
       "max1         NaN\n",
       "mean1        NaN\n",
       "median1      NaN\n",
       "std1         NaN\n",
       "1st quart1   NaN\n",
       "3rd quart1   NaN\n",
       "min2         NaN\n",
       "max2         NaN\n",
       "mean2        NaN\n",
       "median2      NaN\n",
       "std2         NaN\n",
       "1st quart2   NaN\n",
       "3rd quart2   NaN\n",
       "min3         NaN\n",
       "max3         NaN\n",
       "mean3        NaN\n",
       "median3      NaN\n",
       "std3         NaN\n",
       "1st quart3   NaN\n",
       "3rd quart3   NaN\n",
       "min4         NaN\n",
       "max4         NaN\n",
       "mean4        NaN\n",
       "median4      NaN\n",
       "std4         NaN\n",
       "1st quart4   NaN\n",
       "3rd quart4   NaN\n",
       "min5         NaN\n",
       "max5         NaN\n",
       "mean5        NaN\n",
       "median5      NaN\n",
       "std5         NaN\n",
       "1st quart5   NaN\n",
       "3rd quart5   NaN\n",
       "min6         NaN\n",
       "max6         NaN\n",
       "mean6        NaN\n",
       "median6      NaN\n",
       "std6         NaN\n",
       "1st quart6   NaN\n",
       "3rd quart6   NaN\n",
       "Name: 18, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.loc[18]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f94e99ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new combined dataset is : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min1</th>\n",
       "      <th>max1</th>\n",
       "      <th>mean1</th>\n",
       "      <th>median1</th>\n",
       "      <th>std1</th>\n",
       "      <th>1st quart1</th>\n",
       "      <th>3rd quart1</th>\n",
       "      <th>min2</th>\n",
       "      <th>max2</th>\n",
       "      <th>mean2</th>\n",
       "      <th>...</th>\n",
       "      <th>std5</th>\n",
       "      <th>1st quart5</th>\n",
       "      <th>3rd quart5</th>\n",
       "      <th>min6</th>\n",
       "      <th>max6</th>\n",
       "      <th>mean6</th>\n",
       "      <th>median6</th>\n",
       "      <th>std6</th>\n",
       "      <th>1st quart6</th>\n",
       "      <th>3rd quart6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36.25</td>\n",
       "      <td>48.00</td>\n",
       "      <td>43.969125</td>\n",
       "      <td>44.500</td>\n",
       "      <td>1.618364</td>\n",
       "      <td>43.31</td>\n",
       "      <td>44.6700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.413125</td>\n",
       "      <td>...</td>\n",
       "      <td>3.318301</td>\n",
       "      <td>20.5000</td>\n",
       "      <td>23.7500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.555313</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.487826</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.00</td>\n",
       "      <td>48.00</td>\n",
       "      <td>43.454958</td>\n",
       "      <td>43.250</td>\n",
       "      <td>1.386098</td>\n",
       "      <td>42.50</td>\n",
       "      <td>45.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.378083</td>\n",
       "      <td>...</td>\n",
       "      <td>2.488862</td>\n",
       "      <td>22.2500</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.26</td>\n",
       "      <td>0.679646</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.622534</td>\n",
       "      <td>0.4300</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.00</td>\n",
       "      <td>47.75</td>\n",
       "      <td>42.179813</td>\n",
       "      <td>43.500</td>\n",
       "      <td>3.670666</td>\n",
       "      <td>39.15</td>\n",
       "      <td>45.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.696042</td>\n",
       "      <td>...</td>\n",
       "      <td>3.849448</td>\n",
       "      <td>30.4575</td>\n",
       "      <td>36.3300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.613521</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.524317</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.00</td>\n",
       "      <td>45.75</td>\n",
       "      <td>41.678063</td>\n",
       "      <td>41.750</td>\n",
       "      <td>2.243490</td>\n",
       "      <td>41.33</td>\n",
       "      <td>42.7500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.83</td>\n",
       "      <td>0.535979</td>\n",
       "      <td>...</td>\n",
       "      <td>2.411026</td>\n",
       "      <td>28.4575</td>\n",
       "      <td>31.2500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.383292</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.389164</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.00</td>\n",
       "      <td>47.40</td>\n",
       "      <td>43.954500</td>\n",
       "      <td>44.330</td>\n",
       "      <td>1.558835</td>\n",
       "      <td>43.00</td>\n",
       "      <td>45.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.426250</td>\n",
       "      <td>...</td>\n",
       "      <td>1.999604</td>\n",
       "      <td>35.3625</td>\n",
       "      <td>36.5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.493292</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.513506</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>24.75</td>\n",
       "      <td>48.33</td>\n",
       "      <td>44.182937</td>\n",
       "      <td>48.000</td>\n",
       "      <td>7.495615</td>\n",
       "      <td>48.00</td>\n",
       "      <td>48.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.101875</td>\n",
       "      <td>...</td>\n",
       "      <td>3.274539</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>5.5425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.91</td>\n",
       "      <td>0.692771</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.675781</td>\n",
       "      <td>0.3225</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>48.00</td>\n",
       "      <td>48.25</td>\n",
       "      <td>48.004167</td>\n",
       "      <td>48.000</td>\n",
       "      <td>0.032038</td>\n",
       "      <td>48.00</td>\n",
       "      <td>48.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.007167</td>\n",
       "      <td>...</td>\n",
       "      <td>3.268502</td>\n",
       "      <td>4.6700</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.641229</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.388372</td>\n",
       "      <td>0.4600</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>24.25</td>\n",
       "      <td>45.00</td>\n",
       "      <td>37.177042</td>\n",
       "      <td>36.250</td>\n",
       "      <td>3.581301</td>\n",
       "      <td>34.50</td>\n",
       "      <td>40.2500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.58</td>\n",
       "      <td>2.374208</td>\n",
       "      <td>...</td>\n",
       "      <td>2.890347</td>\n",
       "      <td>17.9500</td>\n",
       "      <td>21.7500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.34</td>\n",
       "      <td>2.921729</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.852600</td>\n",
       "      <td>1.5000</td>\n",
       "      <td>3.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>28.75</td>\n",
       "      <td>44.75</td>\n",
       "      <td>37.561188</td>\n",
       "      <td>36.875</td>\n",
       "      <td>3.226507</td>\n",
       "      <td>35.25</td>\n",
       "      <td>40.2500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.91</td>\n",
       "      <td>2.080688</td>\n",
       "      <td>...</td>\n",
       "      <td>2.727377</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>21.5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.62</td>\n",
       "      <td>2.765896</td>\n",
       "      <td>2.45</td>\n",
       "      <td>1.769203</td>\n",
       "      <td>1.4100</td>\n",
       "      <td>3.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>22.00</td>\n",
       "      <td>44.67</td>\n",
       "      <td>37.058708</td>\n",
       "      <td>36.000</td>\n",
       "      <td>3.710180</td>\n",
       "      <td>34.50</td>\n",
       "      <td>40.0625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.17</td>\n",
       "      <td>2.438146</td>\n",
       "      <td>...</td>\n",
       "      <td>3.537144</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>21.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.55</td>\n",
       "      <td>2.983750</td>\n",
       "      <td>2.57</td>\n",
       "      <td>1.815730</td>\n",
       "      <td>1.5000</td>\n",
       "      <td>4.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     min1   max1      mean1  median1      std1  1st quart1  3rd quart1  min2  \\\n",
       "0   36.25  48.00  43.969125   44.500  1.618364       43.31     44.6700   0.0   \n",
       "1   37.00  48.00  43.454958   43.250  1.386098       42.50     45.0000   0.0   \n",
       "2   33.00  47.75  42.179813   43.500  3.670666       39.15     45.0000   0.0   \n",
       "3   33.00  45.75  41.678063   41.750  2.243490       41.33     42.7500   0.0   \n",
       "4   35.00  47.40  43.954500   44.330  1.558835       43.00     45.0000   0.0   \n",
       "..    ...    ...        ...      ...       ...         ...         ...   ...   \n",
       "83  24.75  48.33  44.182937   48.000  7.495615       48.00     48.0000   0.0   \n",
       "84  48.00  48.25  48.004167   48.000  0.032038       48.00     48.0000   0.0   \n",
       "85  24.25  45.00  37.177042   36.250  3.581301       34.50     40.2500   0.0   \n",
       "86  28.75  44.75  37.561188   36.875  3.226507       35.25     40.2500   0.0   \n",
       "87  22.00  44.67  37.058708   36.000  3.710180       34.50     40.0625   0.0   \n",
       "\n",
       "     max2     mean2  ...      std5  1st quart5  3rd quart5  min6  max6  \\\n",
       "0    1.50  0.413125  ...  3.318301     20.5000     23.7500   0.0  2.96   \n",
       "1    1.58  0.378083  ...  2.488862     22.2500     24.0000   0.0  5.26   \n",
       "2    3.00  0.696042  ...  3.849448     30.4575     36.3300   0.0  2.18   \n",
       "3    2.83  0.535979  ...  2.411026     28.4575     31.2500   0.0  1.79   \n",
       "4    1.70  0.426250  ...  1.999604     35.3625     36.5000   0.0  1.79   \n",
       "..    ...       ...  ...       ...         ...         ...   ...   ...   \n",
       "83   3.11  0.101875  ...  3.274539      2.0000      5.5425   0.0  3.91   \n",
       "84   0.43  0.007167  ...  3.268502      4.6700     10.0000   0.0  2.50   \n",
       "85   8.58  2.374208  ...  2.890347     17.9500     21.7500   0.0  9.34   \n",
       "86   9.91  2.080688  ...  2.727377     18.0000     21.5000   0.0  9.62   \n",
       "87  14.17  2.438146  ...  3.537144     16.0000     21.0000   0.0  8.55   \n",
       "\n",
       "       mean6  median6      std6  1st quart6  3rd quart6  \n",
       "0   0.555313     0.49  0.487826      0.0000        0.83  \n",
       "1   0.679646     0.50  0.622534      0.4300        0.87  \n",
       "2   0.613521     0.50  0.524317      0.0000        1.00  \n",
       "3   0.383292     0.43  0.389164      0.0000        0.50  \n",
       "4   0.493292     0.43  0.513506      0.0000        0.94  \n",
       "..       ...      ...       ...         ...         ...  \n",
       "83  0.692771     0.50  0.675781      0.3225        0.94  \n",
       "84  0.641229     0.50  0.388372      0.4600        0.83  \n",
       "85  2.921729     2.50  1.852600      1.5000        3.90  \n",
       "86  2.765896     2.45  1.769203      1.4100        3.77  \n",
       "87  2.983750     2.57  1.815730      1.5000        4.15  \n",
       "\n",
       "[88 rows x 42 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.concat([time1, time2, time3, time4, time5, time6], axis=1)\n",
    "print(\"The new combined dataset is : \\n\")\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c72001",
   "metadata": {},
   "source": [
    "iii. Estimate the standard deviation of each of the time-domain features you extracted from the data. Then, use Python’s bootstrapped or any other method to build a 90% bootsrap confidence interval for the standard deviation of each feature.\n",
    "\n",
    "https://notebook.community/tsarouch/python_minutes/core/Confidence_Prediction_Intervals\n",
    "\n",
    "https://www.statology.org/pandas-standard-deviation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aad7aae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(new_df.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aea60cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90% Bootstrap Confidence Interval\n",
      "                                 Confidence Interval\n",
      "min1         [8.304781657845911, 10.696117094974904]\n",
      "max1        [3.0826492741764855, 5.0809736088805755]\n",
      "mean1         [4.636195598908917, 5.802788810094723]\n",
      "median1       [4.7287860551361725, 5.95455355666042]\n",
      "std1        [1.5548007919051805, 1.9432509788578503]\n",
      "1st quart1    [5.519242376123826, 6.596603587425196]\n",
      "3rd quart1    [4.156338263194703, 5.699967899579457]\n",
      "                                 Confidence Interval\n",
      "min2                                      [0.0, 0.0]\n",
      "max2         [4.590488161973379, 5.3741807294578505]\n",
      "mean2       [1.3869661541475387, 1.7040584524348994]\n",
      "median2       [1.241255352353696, 1.537121623059281]\n",
      "std2        [0.7966382076782204, 0.9407528246304985]\n",
      "1st quart2    [0.828017832037779, 1.032217106563464]\n",
      "3rd quart2  [1.8832933845305897, 2.2795105096025146]\n",
      "                                 Confidence Interval\n",
      "min3        [2.7368861701579754, 3.0864468790378914]\n",
      "max3         [4.1108265716166725, 5.385915067178367]\n",
      "mean3         [3.405493446682145, 4.478353618108552]\n",
      "median3       [3.362163060339306, 4.491820020193433]\n",
      "std3        [0.7554568148907403, 1.1362955501621947]\n",
      "1st quart3   [3.5631390033591623, 4.681396482292874]\n",
      "3rd quart3    [3.4891417765662096, 4.67580366492345]\n",
      "                                 Confidence Interval\n",
      "min4                                      [0.0, 0.0]\n",
      "max4        [1.9564028482586626, 2.3390177094472206]\n",
      "mean4         [1.067768057301467, 1.211270611404374]\n",
      "median4     [1.0565667802432437, 1.1992414744627802]\n",
      "std4        [0.4207063054232279, 0.4832402013335924]\n",
      "1st quart4  [0.7686404950081899, 0.8854134065209868]\n",
      "3rd quart4   [1.423378640351169, 1.6193501903838947]\n",
      "                                 Confidence Interval\n",
      "min5          [4.359389613378811, 7.524684263802191]\n",
      "max5          [4.768452721212532, 6.547917452975302]\n",
      "mean5          [4.356273928103458, 6.71884297234581]\n",
      "median5      [4.5185409596123876, 6.903583065920051]\n",
      "std5        [0.8004898072862686, 1.2001305444202395]\n",
      "1st quart5    [4.690204109521126, 7.174990115955169]\n",
      "3rd quart5    [4.392109552119737, 6.632984814747136]\n",
      "                                 Confidence Interval\n",
      "min6                      [0.0, 0.07802896990623477]\n",
      "max6         [2.247898919978654, 2.7476194792391144]\n",
      "mean6         [1.062826649037147, 1.208184983003948]\n",
      "median6     [0.9912574211501066, 1.1476779337459184]\n",
      "std6        [0.4784749333568823, 0.5421792423489139]\n",
      "1st quart6   [0.684007589963059, 0.8065746224210245]\n",
      "3rd quart6  [1.3957066766618336, 1.5935950008695166]\n"
     ]
    }
   ],
   "source": [
    "df1 = [time1, time2, time3, time4, time5, time6]\n",
    "\n",
    "print('90% Bootstrap Confidence Interval')\n",
    "for feature in df1:\n",
    "    conf = pd.DataFrame(columns = ['Confidence Interval'])\n",
    "    \n",
    "    for col in feature.columns:\n",
    "        alpha = 0.9 \n",
    "        std = []\n",
    "        p = ( ( 1.0 - alpha ) / 2.0) * 100\n",
    "        q = ( alpha + ( ( 1.0 - alpha ) / 2.0 ) ) * 100\n",
    "        leng = len(feature)\n",
    "        \n",
    "        for i in range(1000):\n",
    "            sample = (resample(feature[col], n_samples = leng, replace = True))\n",
    "            std.append(np.std(sample)) \n",
    "            \n",
    "        lower = np.percentile(std, p)\n",
    "        upper = np.percentile(std, q)\n",
    "        conf.loc[col] = str([lower, upper])\n",
    "        \n",
    "    print(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2771b64f",
   "metadata": {},
   "source": [
    "iv. Use your judgement to select the three most important time-domain features (one option may be min, mean, and max).\n",
    "\n",
    "max, mean and standard deviation are the most important time-domain features, in my opinion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ca51cc",
   "metadata": {},
   "source": [
    "# 2. ISLR 3.7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e7115",
   "metadata": {},
   "source": [
    "I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. Y = β0 +β1X +β2X2 +β3X3 +ε.\n",
    "\n",
    "SOLUTIONS : \n",
    "\n",
    "(a) Given the cubic regression gives a larger flexibility than linear regression model, it will be better fit to the data. With increase in model flexibilty, the training mean square error will decreases. Hence, the training RSS for cubic regression would be smaller than linear regression. \n",
    "\n",
    "(b) Having additional predictors / more flexibilty in the model will have higher chance of overfitting. Thus testing RSS for cubic regression will be higher. Linear regression, being a less flexible model, always has less variance than cubic regression. As RSS depends on variance, linear regression will produce a lower test RSS than cubic regression since the variance of is constant and the linear model has lower bias and variance than the cubic model.\n",
    "\n",
    "(c) The cubic regression's training RSS will be lower than that of the linear regression. Again, this is because cubic regression is more flexible and will be better fit to data, and flexibility results in lower training mean squared error and, hence, lower training RSS.\n",
    "\n",
    "(d) Given that the real relationship is not linear, cubic regression will probably have less bias than linear regression. But we don't know how far off from linear it is, so we don't know the bias difference between the models. The accuracy of a cubic regression model is also more dependent on the training data since, cubic regression has a bigger variance than linear regression. We have no knowledge of the training data, therefore we are unsure of how accurately it captures the relationship between the predictor and the response. Therefore, given information is insufficient for this part as you cannot determine the the true relationship of X and Y. We are unsure about their linearity and the regression that the relationship is closest to.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
